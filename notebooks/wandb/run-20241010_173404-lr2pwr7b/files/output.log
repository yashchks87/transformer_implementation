  0%|          | 0/267 [00:00<?, ?batch/s]/databricks/python/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|██████████| 267/267 [02:57<00:00,  1.51batch/s, loss=1.11e+3]
100%|██████████| 3/3 [00:19<00:00,  6.65s/batch, loss=10]  
Epoch 0/50 | Train Loss: 4.144697857260257 | Val Loss: 3.3475104173024497
100%|██████████| 267/267 [02:57<00:00,  1.51batch/s, loss=835]
100%|██████████| 3/3 [00:19<00:00,  6.64s/batch, loss=8.59]
Epoch 1/50 | Train Loss: 3.128815033909087 | Val Loss: 2.862981160481771
100%|██████████| 267/267 [02:52<00:00,  1.54batch/s, loss=730]
100%|██████████| 3/3 [00:20<00:00,  6.92s/batch, loss=7.58]
Epoch 2/50 | Train Loss: 2.7352234352840465 | Val Loss: 2.5251020590464273
100%|██████████| 267/267 [02:52<00:00,  1.55batch/s, loss=646]
100%|██████████| 3/3 [00:19<00:00,  6.44s/batch, loss=6.76]
Epoch 3/50 | Train Loss: 2.420821332752928 | Val Loss: 2.25229541460673
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=578]
100%|██████████| 3/3 [00:19<00:00,  6.58s/batch, loss=6.08]
Epoch 4/50 | Train Loss: 2.165365408422349 | Val Loss: 2.027045965194702
100%|██████████| 267/267 [02:52<00:00,  1.55batch/s, loss=524]
100%|██████████| 3/3 [00:19<00:00,  6.45s/batch, loss=5.56]
Epoch 5/50 | Train Loss: 1.961724693409066 | Val Loss: 1.8532852331797283
100%|██████████| 267/267 [02:52<00:00,  1.55batch/s, loss=480]
100%|██████████| 3/3 [00:18<00:00,  6.32s/batch, loss=5.24]
Epoch 6/50 | Train Loss: 1.7987621312730768 | Val Loss: 1.7450427611668904
100%|██████████| 267/267 [02:52<00:00,  1.54batch/s, loss=444]
100%|██████████| 3/3 [00:19<00:00,  6.49s/batch, loss=4.89]
Epoch 7/50 | Train Loss: 1.661128891987747 | Val Loss: 1.63035249710083
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=411]
100%|██████████| 3/3 [00:20<00:00,  6.94s/batch, loss=4.61]
Epoch 8/50 | Train Loss: 1.5406558852963679 | Val Loss: 1.5361987749735515
 85%|████████▌ | 227/267 [02:21<00:22,  1.77batch/s, loss=327]/databricks/python/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=383]
100%|██████████| 3/3 [00:19<00:00,  6.57s/batch, loss=4.36]
Epoch 9/50 | Train Loss: 1.4334380698114746 | Val Loss: 1.4539061784744263
100%|██████████| 267/267 [02:54<00:00,  1.53batch/s, loss=357]
100%|██████████| 3/3 [00:20<00:00,  6.67s/batch, loss=4.14]
Epoch 10/50 | Train Loss: 1.3379739893509655 | Val Loss: 1.3783890008926392
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=333]
100%|██████████| 3/3 [00:20<00:00,  6.98s/batch, loss=3.95]
Epoch 11/50 | Train Loss: 1.248859079589558 | Val Loss: 1.3159011999766033
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=312]
100%|██████████| 3/3 [00:20<00:00,  6.68s/batch, loss=3.76]
Epoch 12/50 | Train Loss: 1.1668907323580109 | Val Loss: 1.2549803256988525
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=291]
100%|██████████| 3/3 [00:19<00:00,  6.42s/batch, loss=3.62]
Epoch 13/50 | Train Loss: 1.0916080099813055 | Val Loss: 1.2050555149714153
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=273]
100%|██████████| 3/3 [00:19<00:00,  6.48s/batch, loss=3.46]
Epoch 14/50 | Train Loss: 1.021786076745737 | Val Loss: 1.153994083404541
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=255] 
100%|██████████| 3/3 [00:19<00:00,  6.35s/batch, loss=3.3] 
Epoch 15/50 | Train Loss: 0.9562492667512501 | Val Loss: 1.100804328918457
100%|██████████| 267/267 [02:56<00:00,  1.51batch/s, loss=239] 
100%|██████████| 3/3 [00:22<00:00,  7.42s/batch, loss=3.23]
Epoch 16/50 | Train Loss: 0.8940305638402589 | Val Loss: 1.076746940612793
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=223] 
100%|██████████| 3/3 [00:19<00:00,  6.51s/batch, loss=3.08] 
Epoch 17/50 | Train Loss: 0.83693459016107 | Val Loss: 1.026960829893748
100%|██████████| 267/267 [02:54<00:00,  1.53batch/s, loss=208] 
100%|██████████| 3/3 [00:19<00:00,  6.57s/batch, loss=2.98]
Epoch 18/50 | Train Loss: 0.7804722294825294 | Val Loss: 0.993267277876536
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=195] 
100%|██████████| 3/3 [00:19<00:00,  6.66s/batch, loss=2.91] 
Epoch 19/50 | Train Loss: 0.7291646095250877 | Val Loss: 0.9708462158838908
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=182] 
100%|██████████| 3/3 [00:19<00:00,  6.54s/batch, loss=2.8]  
Epoch 20/50 | Train Loss: 0.6813435174999166 | Val Loss: 0.9345980683962504
 11%|█         | 29/267 [00:28<02:14,  1.76batch/s, loss=18.6]/databricks/python/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|██████████| 267/267 [02:58<00:00,  1.50batch/s, loss=170] 
100%|██████████| 3/3 [00:20<00:00,  6.85s/batch, loss=2.72] 
Epoch 21/50 | Train Loss: 0.6362913364774725 | Val Loss: 0.9055080612500509
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=158] 
100%|██████████| 3/3 [00:20<00:00,  6.70s/batch, loss=2.65]
Epoch 22/50 | Train Loss: 0.5921356780698683 | Val Loss: 0.8821162184079488
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=147] 
100%|██████████| 3/3 [00:19<00:00,  6.43s/batch, loss=2.6]  
Epoch 23/50 | Train Loss: 0.5523631532540482 | Val Loss: 0.8680696288744608
100%|██████████| 267/267 [02:52<00:00,  1.54batch/s, loss=137] 
100%|██████████| 3/3 [00:20<00:00,  6.68s/batch, loss=2.5]  
Epoch 24/50 | Train Loss: 0.5139877071764585 | Val Loss: 0.8339243133862814
100%|██████████| 267/267 [02:52<00:00,  1.55batch/s, loss=128] 
100%|██████████| 3/3 [00:19<00:00,  6.62s/batch, loss=2.52] 
Epoch 25/50 | Train Loss: 0.47916575417982954 | Val Loss: 0.8385394811630249
100%|██████████| 267/267 [02:54<00:00,  1.53batch/s, loss=119] 
100%|██████████| 3/3 [00:20<00:00,  6.74s/batch, loss=2.44] 
Epoch 26/50 | Train Loss: 0.44627318109912373 | Val Loss: 0.8120211164156595
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=111] 
100%|██████████| 3/3 [00:19<00:00,  6.66s/batch, loss=2.38] 
Epoch 27/50 | Train Loss: 0.4154577837901169 | Val Loss: 0.7932316660881042
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=103] 
100%|██████████| 3/3 [00:19<00:00,  6.63s/batch, loss=2.36] 
Epoch 28/50 | Train Loss: 0.3851434760325857 | Val Loss: 0.7866793076197306
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=95.7]
100%|██████████| 3/3 [00:20<00:00,  6.79s/batch, loss=2.31] 
Epoch 29/50 | Train Loss: 0.35829414857014286 | Val Loss: 0.7696177959442139
100%|██████████| 267/267 [02:52<00:00,  1.55batch/s, loss=89]  
100%|██████████| 3/3 [00:20<00:00,  6.80s/batch, loss=2.27] 
Epoch 30/50 | Train Loss: 0.3332203049561504 | Val Loss: 0.7554578185081482
100%|██████████| 267/267 [02:57<00:00,  1.51batch/s, loss=82.8]
100%|██████████| 3/3 [00:19<00:00,  6.44s/batch, loss=2.25] 
Epoch 31/50 | Train Loss: 0.31007538373104193 | Val Loss: 0.7495858073234558
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=77]  
100%|██████████| 3/3 [00:19<00:00,  6.58s/batch, loss=2.23] 
Epoch 32/50 | Train Loss: 0.28845961442154444 | Val Loss: 0.7422332366307577
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=71.6]
100%|██████████| 3/3 [00:20<00:00,  6.87s/batch, loss=2.23] 
Epoch 33/50 | Train Loss: 0.2679814561252737 | Val Loss: 0.7432397603988647
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=66.7]
100%|██████████| 3/3 [00:21<00:00,  7.21s/batch, loss=2.2]  
Epoch 34/50 | Train Loss: 0.2497047254543626 | Val Loss: 0.7325910727183024
100%|██████████| 267/267 [02:56<00:00,  1.52batch/s, loss=61.8]
100%|██████████| 3/3 [00:19<00:00,  6.56s/batch, loss=2.18] 
Epoch 35/50 | Train Loss: 0.23155385892042954 | Val Loss: 0.7281614343325297
100%|██████████| 267/267 [02:55<00:00,  1.52batch/s, loss=57.4]
100%|██████████| 3/3 [00:19<00:00,  6.44s/batch, loss=2.16] 
Epoch 36/50 | Train Loss: 0.21511720528316855 | Val Loss: 0.7193530400594076
100%|██████████| 267/267 [02:54<00:00,  1.53batch/s, loss=53.5]
100%|██████████| 3/3 [00:19<00:00,  6.66s/batch, loss=2.13] 
Epoch 37/50 | Train Loss: 0.20032216056009358 | Val Loss: 0.710997740427653
100%|██████████| 267/267 [02:58<00:00,  1.50batch/s, loss=49.8]
100%|██████████| 3/3 [00:19<00:00,  6.50s/batch, loss=2.15] 
Epoch 38/50 | Train Loss: 0.18660196617301483 | Val Loss: 0.7180696725845337
100%|██████████| 267/267 [02:52<00:00,  1.54batch/s, loss=46.6]
100%|██████████| 3/3 [00:19<00:00,  6.62s/batch, loss=2.15] 
Epoch 39/50 | Train Loss: 0.17440266856986486 | Val Loss: 0.7156508962313334
100%|██████████| 267/267 [02:54<00:00,  1.53batch/s, loss=43.2]
100%|██████████| 3/3 [00:19<00:00,  6.53s/batch, loss=2.12] 
Epoch 40/50 | Train Loss: 0.16191154651427536 | Val Loss: 0.7069747845331827
100%|██████████| 267/267 [02:56<00:00,  1.52batch/s, loss=40.5]
100%|██████████| 3/3 [00:19<00:00,  6.50s/batch, loss=2.13] 
Epoch 41/50 | Train Loss: 0.15164326242963028 | Val Loss: 0.7085910240809122
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=37.5]
100%|██████████| 3/3 [00:24<00:00,  8.03s/batch, loss=2.12] 
Epoch 42/50 | Train Loss: 0.14038837449939062 | Val Loss: 0.7079747915267944
 23%|██▎       | 62/267 [00:47<01:56,  1.76batch/s, loss=7.78]/databricks/python/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|██████████| 267/267 [02:53<00:00,  1.54batch/s, loss=35.2]
100%|██████████| 3/3 [00:19<00:00,  6.42s/batch, loss=2.14] 
Epoch 43/50 | Train Loss: 0.1317610422546944 | Val Loss: 0.7130610148111979
100%|██████████| 267/267 [02:54<00:00,  1.53batch/s, loss=33]  
100%|██████████| 3/3 [00:19<00:00,  6.45s/batch, loss=2.14] 
Epoch 44/50 | Train Loss: 0.12359133189760343 | Val Loss: 0.7116989294687907
  0%|          | 0/267 [00:06<?, ?batch/s]
